{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!git clone https://github.com/sithu31296/semantic-segmentation\r\n",
    "%cd semantic-segmentation\r\n",
    "%pip install -r requirements.txt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\r\n",
    "import random\r\n",
    "from torchvision import io\r\n",
    "from torchvision import transforms as T\r\n",
    "from PIL import Image\r\n",
    "\r\n",
    "def show_image(image):\r\n",
    "    if image.shape[2] != 3: image = image.permute(1, 2, 0)\r\n",
    "    image = Image.fromarray(image.numpy())\r\n",
    "    return image"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Show Available Pretrained Models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from utils.utils import show_models\r\n",
    "\r\n",
    "show_models()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load a Pretrained Model\n",
    "\n",
    "Download a pretrained model's weights from the result table (ADE20K, CityScapes, ...) and put it in `checkpoints/pretrained/model_name/`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%pip install gdown"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import gdown\r\n",
    "from pathlib import Path\r\n",
    "\r\n",
    "ckpt = Path('./checkpoints/pretrained/segformer')\r\n",
    "ckpt.mkdir(exist_ok=True, parents=True)\r\n",
    "\r\n",
    "url = 'https://drive.google.com/uc?id=1-OmW3xRD3WAbJTzktPC-VMOF5WMsN8XT'\r\n",
    "output = './checkpoints/pretrained/segformer/segformer.b3.ade.pth'\r\n",
    "\r\n",
    "gdown.download(url, output, quiet=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from models import get_model\r\n",
    "\r\n",
    "model = get_model(\r\n",
    "    model_name='SegFormer',\r\n",
    "    variant='B3',\r\n",
    "    num_classes=150     # ade20k\r\n",
    ")\r\n",
    "try:\r\n",
    "    model.load_state_dict(torch.load('checkpoints/pretrained/segformer/segformer.b3.ade.pth', map_location='cpu'))\r\n",
    "except:\r\n",
    "    print(\"Download a pretrained model's weights from the result table.\")\r\n",
    "model.eval()\r\n",
    "\r\n",
    "print('Loaded Model')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simple Image Inference\n",
    "\n",
    "### Load Image"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "image_path = 'assests/ade/ADE_val_00000049.jpg'\r\n",
    "image = io.read_image(image_path)\r\n",
    "print(image.shape)\r\n",
    "show_image(image)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocess"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# resize\r\n",
    "image = T.CenterCrop((512, 512))(image)\r\n",
    "# scale to [0.0, 1.0]\r\n",
    "image = image.float() / 255\r\n",
    "# normalize\r\n",
    "image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))(image)\r\n",
    "# add batch size\r\n",
    "image = image.unsqueeze(0)\r\n",
    "image.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Forward"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with torch.no_grad():\r\n",
    "    seg = model(image)\r\n",
    "seg.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Postprocess"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "seg = seg.softmax(1).argmax(1).to(int)\r\n",
    "seg.unique()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from datasets import ADE20K\r\n",
    "\r\n",
    "palette = ADE20K.PALETTE"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "seg_map = palette[seg].squeeze().to(torch.uint8)\r\n",
    "show_image(seg_map)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Show Available Backbones"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from models import backbones\r\n",
    "\r\n",
    "backbones.__all__"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Show Available Heads"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from models import heads\r\n",
    "\r\n",
    "heads.__all__"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Construct a Custom Model\n",
    "\n",
    "### Choose a Backbone"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from models.backbones import ResNet\r\n",
    "\r\n",
    "backbone = ResNet('18')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# init random input batch\r\n",
    "x = torch.randn(2, 3, 224, 224)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get features from the backbone\r\n",
    "features = backbone(x)\r\n",
    "for out in features:\r\n",
    "    print(out.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Choose a Head"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from models.heads import UPerHead\r\n",
    "\r\n",
    "head = UPerHead([64, 128, 256, 512], 128, num_classes=10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "seg = head(features)\r\n",
    "seg.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torch.nn import functional as F\r\n",
    "# upsample the output\r\n",
    "seg = F.interpolate(seg, size=x.shape[-2:], mode='bilinear', align_corners=False)\r\n",
    "seg.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check `models/custom_cnn.py` and `models/custom_vit.py` for a complete construction for custom model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "interpreter": {
   "hash": "78184fe1b8a3f830767e8814b2b01c36fc7c8ac521e39cb583cd3fce210fee57"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}